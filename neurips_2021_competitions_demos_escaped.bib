@Proceedings{NEURIPS-DEMOS-COMPETITIONS-2021,
    booktitle = {Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track},
    name = {NeurIPS 2021 Competitions and Demonstrations Track},
    shortname = {NeurIPS 2021: Competitions and Demonstrations},
    editor = {Douwe Kiela and Marco Ciccone and Barbara Caputo},
    volume = {176},
    year = {2022},
    start = {2022-12-6},
    end = {2022-12-14},
    published = {2022-07-20},
    conference_url = {https://neurips.cc/Conferences/2021},
    address = {Online},
    sections = {Preface|Competitions|Demonstrations},
}

@InProceedings{Kiela22,
    title = {NeurIPS 2021 Competition and Demonstration Track Revised Selected Papers},
    author = {Douwe Kiela and Marco Ciccone and Barbara Caputo},
    pages = {i-ii},
    abstract = {Introduction to this volume.},
    section = {Preface}
}

@InProceedings{Papakipos22,
 title = {Results and findings of the 2021 Image Similarity Challenge},
 author = {Papakipos, Zo\"e and Tolias, Giorgos and Jenicek, Tomas and Pizzi, Ed and Yokoo, Shuhei and Wang, Wenhao and Sun, Yifan and Zhang, Weipu and Yang, Yi and Addicam, Sanjay and Papadakis, Sergio Manuel and Ferrer, Cristian Canton and Chum, Ond{\v{r}}ej and Douze, Matthijs},
 pages = {1-12},
 abstract = {The 2021 Image Similarity Challenge introduced a dataset to serve as a benchmark to evaluate image copy detection methods. There were 200 participants to the competition. This paper presents a quantitative and qualitative analysis of the top submissions. It appears that the most difficult image transformations involve either severe image crops or overlaying onto unrelated images, combined with local pixel perturbations. The key algorithmic elements in the winning submissions are: training on strong augmentations, self-supervised learning, score normalization, explicit overlay detection, and global descriptor matching followed by pairwise image comparison.},
 section = {Competitions}
}

@InProceedings{Kanervisto22,
 title = {MineRL Diamond 2021 Competition: Overview, Results, and Lessons Learned},
 author = {Kanervisto, Anssi and Milani, Stephanie and Ramanauskas, Karolis and Topin, Nicholay and Lin, Zichuan and Li, Junyou and Shi, Jianing and Ye, Deheng and Fu, Qiang and Yang, Wei and Hong, Weijun and Huang, Zhongyue and Chen, Haicheng and Zeng, Guangjun and Lin, Yue and Micheli, Vincent and Alonso, Eloi and Fleuret, Fran\c{c}ois and Nikulin, Alexander and Belousov, Yury and Svidchenko, Oleg and Shpilman, Aleksei},
 pages = {13-28},
 abstract = {Reinforcement learning competitions advance the field by providing appropriate scope and support to develop solutions toward a specific problem. To promote the development of more broadly applicable methods, organizers need to enforce the use of general techniques, the use of sample-efficient methods, and the reproducibility of the results. While beneficial for the research community, these restrictions come at a cost---increased difficulty. If the barrier for entry is too high, many potential participants are demoralized. With this in mind, we hosted the third edition of the MineRL ObtainDiamond competition, MineRL Diamond 2021, with a separate track in which we permitted any solution to promote the participation of newcomers. With this track and more extensive tutorials and support, we saw an increased number of submissions. The participants of this easier track were able to obtain a diamond, and the participants of the harder track progressed the generalizable solutions in the same task.},
 section = {Competitions}
}

@InProceedings{Das22,
 title = {The Open Catalyst Challenge 2021: Competition Report},
 author = {Das, Abhishek and Shuaibi, Muhammed and Palizhati, Aini and Goyal, Siddharth and Grover, Aditya and Kolluru, Adeesh and Lan, Janice and Rizvi, Ammar and Sriram, Anuroop and Wood, Brandon and Parikh, Devi and Ulissi, Zachary and Zitnick, C. Lawrence and Ke, Guolin and Zheng, Shuxin and Shi, Yu and He, Di and Liu, Tie-Yan and Ying, Chengxuan and You, Jiacheng and He, Yihan and Grigoriev, Rostislav and Lukin, Ruslan and Yarullin, Adel and Faleev, Max},
 pages = {29-40},
 abstract = {In this report, we describe the Open Catalyst Challenge held at NeurIPS 2021, focusing on using machine learning (ML) to accelerate the search for low-cost catalysts that can drive reactions converting renewable energy to storable forms. Specifically, the challenge required participants to develop ML approaches for relaxed energy prediction, i.e. given atomic positions for an adsorbate-catalyst system, the goal was to predict the energy of the system's relaxed or lowest energy state. To perform well on this task, ML approaches need to approximate the quantum mechanical computations in Density Functional Theory (DFT). By modeling these accurately, the catalyst's impact on the overall rate of a chemical reaction may be estimated; a key factor in filtering potential electrocatalyst materials. The challenge encouraged community-wide progress on this task and the winning approach improved direct relaxed energy prediction by ~15\% relative over the previous state-of-the-art.},
 section = {Competitions}
}

@InProceedings{Hambro22,
 title = {Insights From the NeurIPS 2021 NetHack Challenge},
 author = {Hambro, Eric and Mohanty, Sharada and Babaev, Dmitrii and Byeon, Minwoo and Chakraborty, Dipam and Grefenstette, Edward and Jiang, Minqi and Daejin, Jo and Kanervisto, Anssi and Kim, Jongmin and Kim, Sungwoong and Kirk, Robert and Kurin, Vitaly and K{\"u}ttler, Heinrich and Kwon, Taehwon and Lee, Donghoon and Mella, Vegard and Nardelli, Nantas and Nazarov, Ivan and Ovsov, Nikita and Holder, Jack and Raileanu, Roberta and Ramanauskas, Karolis and Rockt{\"a}schel, Tim and Rothermel, Danielle and Samvelyan, Mikayel and Sorokin, Dmitry and Sypetkowski, Maciej and Sypetkowski, Micha\l{} },
 pages = {41-52},
 abstract = {In this report, we summarize the takeaways from the first NeurIPS 2021 NetHack Challenge. Participants were tasked with developing a program or agent that can win (i.e., 'ascend' in) the popular dungeon-crawler game of NetHack by interacting with the NetHack Learning Environment (NLE), a scalable, procedurally generated, and challenging Gym environment for reinforcement learning (RL). The challenge showcased community-driven progress in AI with many diverse approaches significantly beating the previously best results on NetHack. Furthermore, it served as a direct comparison between neural (e.g., deep RL) and symbolic AI, as well as hybrid systems, demonstrating that on NetHack symbolic bots currently outperform deep RL by a large margin. Lastly, no agent got close to winning the game, illustrating NetHack's suitability as a long-term benchmark for AI research.},
 section = {Competitions}
}

@InProceedings{Perrotta22,
 title = {The Second NeurIPS Tournament of Reconnaissance Blind Chess},
 author = {Perrotta, Gino and Gardner, Ryan W. and Lowman, Corey and Taufeeque, Mohammad and Tongia, Nitish and Kalyanakrishnan, Shivaram and Clark, Gregory and Wang, Kevin and Rothberg, Eitan and Garrison, Brady P. and Dasgupta, Prithviraj and Canavan, Callum and McCabe, Lucas},
 pages = {53-65},
 abstract = {Reconnaissance Blind Chess is an imperfect-information variant of chess with significant private information that challenges state-of-the-art algorithms. The Johns Hopkins University Applied Physics Laboratory and several organizing partners held the second NeurIPS machine Reconnaissance Blind Chess competition in 2021. 18 bots competed in 9,180 games, revealing a dominant champion with 91\% wins. The top four bots in the tournament matched or exceeded the performance of the inaugural tournament's winner. However, none of the algorithms converge to an optimal, unexploitable strategy or appear to have addressed the core research challenges associated with Reconnaissance Blind Chess.},
 section = {Competitions}
}

@InProceedings{Bashkirova22,
 title = {VisDA-2021 Competition: Universal Domain Adaptation to Improve Performance on Out-of-Distribution Data},
 author = {Bashkirova, Dina and Hendrycks, Dan and Kim, Donghyun and Liao, Haojin and Mishra, Samarth and Rajagopalan, Chandramouli and Saenko, Kate and Saito, Kuniaki and Tayyab, Burhan Ul and Teterwak, Piotr and Usman, Ben},
 pages = {66-79},
 abstract = {Progress in machine learning is typically measured by training and testing a model on samples drawn from the same distribution, i.e. the same domain. This over-estimates future accuracy on out-of-distribution data. The Visual Domain Adaptation (VisDA) 2021 competition tests models' ability to adapt to novel test distributions and handle distributional shift. We set up unsupervised domain adaptation challenges for image classifiers and evaluate adaptation to novel viewpoints, backgrounds, styles and degradation in quality. Our challenge draws on large-scale publicly available datasets but constructs the evaluation across domains, rather than the traditional in-domain benchmarking. Furthermore, we focus on the difficult ``universal" setting where, in addition to input distribution drift, methods encounter missing and/or novel classes in the test set. In this paper, we describe the datasets and evaluation metrics and highlight similarities across top-performing methods that might point to promising future directions in universal domain adaptation research. We hope that the competition will encourage further improvement in machine learning methods' ability to handle realistic data in many deployment scenarios. http://ai.bu.edu/visda-2021/.},
 section = {Competitions}
}

@InProceedings{ElBaz22,
 title = {Lessons learned from the NeurIPS 2021 MetaDL challenge: Backbone fine-tuning without episodic meta-learning dominates for few-shot learning image classification},
 author = {El Baz, Adrian and Ullah, Ihsan and Alcoba\c{c}a, Edesio and Carvalho, Andr\'{e} C. P. L. F. and Chen, Hong and Ferreira, Fabio and Gouk, Henry and Guan, Chaoyu and Guyon, Isabelle and Hospedales, Timothy and Hu, Shell and Huisman, Mike and Hutter, Frank and Liu, Zhengying and Mohr, Felix and \"Ozt\"urk, Ekrem and van Rijn, Jan N. and Sun, Haozhe and Wang, Xin and Zhu, Wenwu},
 pages = {80-96},
 abstract = {Although deep neural networks are capable of achieving performance superior to humans on various tasks, they are notorious for requiring large amounts of data and computing resources, restricting their success to domains where such resources are available. Meta-learning methods can address this problem by transferring knowledge from related tasks, thus reducing the amount of data and computing resources needed to learn new tasks. We organize the MetaDL competition series, which provide opportunities for research groups all over the world to create and experimentally assess new meta-(deep)learning solutions for real problems. In this paper, authored collaboratively between the competition organizers and the top-ranked participants, we describe the design of the competition, the datasets, the best experimental results, as well as the top-ranked methods in the NeurIPS 2021 challenge, which attracted 15 active teams who made it to the final phase (by outperforming the baseline), making over 100 code submissions during the feedback phase. The solutions of the top participants have been open-sourced. The lessons learned include that learning good representations is essential for effective transfer learning.},
 section = {Competitions}
}

@InProceedings{Eichenberger22,
 title = {Traffic4cast at NeurIPS 2021 - Temporal and Spatial Few-Shot Transfer Learning in Gridded Geo-Spatial Processes},
 author = {Eichenberger, Christian and Neun, Moritz and Martin, Henry and Herruzo, Pedro and Spanring, Markus and Lu, Yichao and Choi, Sungbin and Konyakhin, Vsevolod and Lukashina, Nina and Shpilman, Aleksei and Wiedemann, Nina and Raubal, Martin and Wang, Bo and Vu, Hai L. and Mohajerpoor, Reza and Cai, Chen and Kim, Inhi and Hermes, Luca and Melnik, Andrew and Velioglu, Riza and Vieth, Markus and Schilling, Malte and Bojesomo, Alabi and Marzouqi, Hasan Al and Liatsis, Panos and Santokhi, Jay and Hillier, Dylan and Yang, Yiming and Sarwar, Joned and Jordan, Anna and Hewage, Emil and Jonietz, David and Tang, Fei and Gruca, Aleksandra and Kopp, Michael and Kreil, David and Hochreiter, Sepp},
 pages = {97-112},
 abstract = {The IARAI Traffic4cast competitions at NeurIPS 2019 and 2020 showed that neural networks can successfully predict future traffic conditions 1 hour into the future on simply aggregated GPS probe data in time and space bins. We thus reinterpreted the challenge of forecasting traffic conditions as a movie completion task. U-Nets proved to be the winning architecture, demonstrating an ability to extract relevant features in this complex real-world geo-spatial process. Building on the previous competitions, Traffic4cast 2021 now focuses on the question of model robustness and generalizability across time and space. Moving from one city to an entirely different city, or moving from pre-COVID times to times after COVID hit the world thus introduces a clear domain shift. We thus, for the first time, release data featuring such domain shifts. The competition now covers ten cities over 2 years, providing data compiled from over $10^{12}$ GPS probe data. Winning solutions captured traffic dynamics sufficiently well to even cope with these complex domain shifts. Surprisingly, this seemed to require only the previous 1h traffic dynamic history and static road graph as input. },
 section = {Competitions}
}

@InProceedings{Wilson22,
 title = {Evaluating Approximate Inference in Bayesian Deep Learning},
 author = {Wilson, Andrew Gordon and Izmailov, Pavel and Hoffman, Matthew D and Gal, Yarin and Li, Yingzhen and Pradier, Melanie F and Vikram, Sharad and Foong, Andrew and Lotfi, Sanae and Farquhar, Sebastian},
 pages = {113-124},
 abstract = {Uncertainty representation is crucial to the safe and reliable deployment of deep learning. Bayesian methods provide a natural mechanism to represent epistemic uncertainty, leading to improved generalization and calibrated predictive distributions. Understanding the fidelity of approximate inference has extraordinary value beyond the standard approach of measuring generalization on a particular task: if approximate inference is working correctly, then we can expect more reliable and accurate deployment across any number of real-world settings. In this competition, we evaluate the fidelity of approximate Bayesian inference procedures in deep learning, using as a reference Hamiltonian Monte Carlo (HMC) samples obtained by parallelizing computations over hundreds of tensor processing unit (TPU) devices. We consider a variety of tasks, including image recognition, regression, covariate shift, and medical applications. All data are publicly available, and we release several baselines, including stochastic MCMC, variational methods, and deep ensembles. The competition resulted in hundreds of submissions across many teams. The winning entries all involved novel multi-modal posterior approximations, highlighting the relative importance of representing multiple modes, and suggesting that we should not consider deep ensembles a {``}non-Bayesian{''} alternative to standard unimodal approximations. In the future, the competition will provide a foundation for innovation and continued benchmarking of approximate Bayesian inference procedures in deep learning. The HMC samples will remain available through the competition website.},
 section = {Competitions}
}

@InProceedings{Turian22,
 title = {HEAR: Holistic Evaluation of Audio Representations},
 author = {Turian, Joseph and Shier, Jordie and Khan, Humair Raj and Raj, Bhiksha and Schuller, Bj\"{o}rn W. and Steinmetz, Christian J. and Malloy, Colin and Tzanetakis, George and Velarde, Gissel and McNally, Kirk and Henry, Max and Pinto, Nicolas and Noufi, Camille and Clough, Christian and Herremans, Dorien and Fonseca, Eduardo and Engel, Jesse and Salamon, Justin and Esling, Philippe and Manocha, Pranay and Watanabe, Shinji and Jin, Zeyu and Bisk, Yonatan},
 pages = {125-145},
 abstract = {What audio embedding approach generalizes best to a wide range of downstream tasks across a variety of everyday domains without fine-tuning? The aim of the HEAR benchmark is to develop a general-purpose audio representation that provides a strong basis for learning in a wide variety of tasks and scenarios. HEAR evaluates audio representations using a benchmark suite across a variety of domains, including speech, environmental sound, and music. HEAR was launched as a NeurIPS 2021 shared challenge. In the spirit of shared exchange, each participant submitted an audio embedding model following a common API that is general-purpose, open-source, and freely available to use. Twenty-nine models by thirteen external teams were evaluated on nineteen diverse downstream tasks derived from sixteen datasets. Open evaluation code, submitted models and datasets are key contributions, enabling comprehensive and reproducible evaluation, as well as previously impossible longitudinal studies. It still remains an open question whether one single general-purpose audio representation can perform as holistically as the human ear.},
 section = {Competitions}
}

@InProceedings{Kiseleva22,
 title = {Interactive Grounded Language Understanding  in a Collaborative Environment: IGLU 2021},
 author = {Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam,Arthur and Sun,Yuxuan and  Hofmann,Katja and  C{\^o}t{\'e},Marc-Alexandre and Awadallah,Ahmed and Abdrazakov, Linar and Churin,Igor and Manggala,Putra and Naszadi,Kata  and  van der Meer, Michiel and  Kim,Taewoon},
 pages = {146-161},
 abstract = {Human intelligence has the remarkable ability to quickly adapt to new tasks and environments. Starting from a very young age, humans acquire new skills and learn how to solve new tasks either by imitating the behavior of others or by following provided natural language instructions. To facilitate research in this direction, we propose IGLU: Interactive Grounded Language Understanding in a Collaborative Environment. The primary goal of the competition is to approach the problem of how to build interactive agents that learn to solve a task while provided with grounded natural language instructions in a collaborative environment. Understanding the complexity of the challenge, we split it into sub-tasks to make it feasible for participants.},
 section = {Competitions}
}

@InProceedings{Lance22,
 title = {Multimodal single cell data integration challenge: Results and lessons learned},
 author = {Christopher Lance and Malte D. Luecken and Daniel B. Burkhardt and Robrecht Cannoodt and Pia Rautenstrauch and Anna Laddach and Aidyn Ubingazhibov and Zhi-Jie Cao and Kaiwen Deng and Sumeer Khan and Qiao Liu and Nikolay Russkikh and Gleb Ryazantsev and Uwe Ohler and NeurIPS 2021 Multimodal data integration competition participants and Angela Oliveira Pisco and Jonathan Bloom and Smita Krishnaswamy and Fabian J. Theis},
 pages = {162-176},
 abstract = {Biology has become a data-intensive science. Recent technological advances in single-cell genomics have enabled the measurement of multiple facets of cellular state, producing datasets with millions of single-cell observations. While these data hold great promise for understanding molecular mechanisms in health and disease, analysis challenges arising from sparsity, technical and biological variability, and high dimensionality of the data hinder the derivation of such mechanistic insights. To promote the innovation of algorithms for analysis of multimodal single-cell data, we organized a competition at NeurIPS 2021 applying the Common Task Framework to multimodal single-cell data integration. For this competition we generated the first multimodal benchmarking dataset for single-cell biology and defined three tasks in this domain: prediction of missing modalities, aligning modalities, and learning a joint representation across modalities. We further specified evaluation metrics and developed a cloud-based algorithm evaluation pipeline. Using this setup, 280 competitors submitted over 2600 proposed solutions within a 3 month period, showcasing substantial innovation especially in the modality alignment task. Here, we present the results, describe trends of well performing approaches, and discuss challenges associated with running the competition.},
 section = {Competitions}
}

@InProceedings{Simhadri22,
 title = {Results of the NeurIPS{'}21 Challenge on Billion-Scale Approximate Nearest Neighbor Search},
 author = {Simhadri, Harsha Vardhan and Williams, George and Aum\"uller, Martin and Douze, Matthijs and Babenko, Artem and Baranchuk, Dmitry and Chen, Qi and Hosseini, Lucas and Krishnaswamny, Ravishankar and Srinivasa, Gopal and Subramanya, Suhas Jayaram and Wang, Jingdong},
 pages = {177-189},
 abstract = {Despite the broad range of algorithms for Approximate Nearest Neighbor Search, most empirical evaluations of algorithms have focused on smaller datasets, typically of 1 million points~\citep{Benchmark}. However, deploying recent advances in embedding based techniques for search, recommendation and ranking at scale require ANNS indices at billion, trillion or larger scale. Barring a few recent papers, there is limited consensus on which algorithms are effective at this scale vis-\`a-vis their hardware cost. This competition\footnote{\url{https://big-ann-benchmarks.com}} compares ANNS algorithms at billion-scale by hardware cost, accuracy and performance. We set up an open source evaluation framework\footnote{\url{https://github.com/harsha-simhadri/big-ann-benchmarks/}}\% and leaderboards for both standardized and specialized hardware. The competition involves three tracks. The standard hardware track T1 evaluates algorithms on an Azure VM with limited DRAM, often the bottleneck in serving billion-scale indices, where the embedding data can be hundreds of GigaBytes in size. It uses FAISS~\citep{Faiss17} as the baseline. The standard hardware track T2 additional allows inexpensive SSDs in addition to the limited DRAM and uses DiskANN~\citep{DiskANN19} as the baseline. The specialized hardware track T3 allows any hardware configuration, and again uses FAISS as the baseline. We compiled six diverse billion-scale datasets, four newly released for this competition, that span a variety of modalities, data types, dimensions, deep learning models, distance functions and sources. The outcome of the competition was ranked leaderboards of algorithms in each track based on recall at a query throughput threshold. Additionally, for track T3, separate leaderboards were created based on recall as well as cost-normalized and power-normalized query throughput.},
 section = {Competitions}
}

@InProceedings{Bauer22,
 title = {Real Robot Challenge: A Robotics Competition in the Cloud},
 author = {Bauer, Stefan and W{\"u}thrich, Manuel and Widmaier, Felix and Buchholz, Annika and Stark, Sebastian and Goyal, Anirudh and Steinbrenner, Thomas and Akpo, Joel and Joshi, Shruti and Berenz, Vincent and Agrawal, Vaibhav and Funk, Niklas and Urain De Jesus, Julen and Peters, Jan and Watson, Joe and Chen, Claire and Srinivasan, Krishnan and Zhang, Junwu and Zhang, Jeffrey and Walter, Matthew and Madan, Rishabh and Yoneda, Takuma and Yarats, Denis and Allshire, Arthur and Gordon, Ethan and Bhattacharjee, Tapomayukh and  Srinivasa, Siddhartha and Garg, Animesh and Maeda, Takahiro and Sikchi, Harshit and Wang, Jilong and Yao, Qingfeng and Yang, Shuyu and McCarthy, Robert and Sanchez, Francisco and Wang, Qiang  and Bulens, David and  McGuinness, Kevin and O'Connor, Noel and Stephen, Redmond and Sch{\"o}lkopf, Bernhard},
 pages = {190-204},
 abstract = {Dexterous manipulation remains an open problem in robotics. To coordinate efforts of the research community towards tackling this problem, we propose a shared benchmark. We designed and built robotic platforms that are hosted at the MPI-IS and can be accessed remotely. Each platform consists of three robotic fingers that are capable of dexterous object manipulation. Users are able to control the platforms remotely by submitting code that is executed automatically, akin to a computational cluster. Using this setup, i) we host robotics competitions, where teams from anywhere in the world access our platforms to tackle challenging tasks ii) we publish the datasets collected during these competitions (consisting of hundreds of robot hours), and iii) we give researchers access to these platforms for their own projects.},
 section = {Competitions}
}

@InProceedings{Wei22,
 title = {2021 BEETL Competition: Advancing Transfer Learning for Subject Independence \{\&} Heterogenous EEG Data Sets},
 author = {Wei, Xiaoxi and Faisal, A. Aldo and Grosse-Wentrup, Moritz and Gramfort, Alexandre and Chevallier, Sylvain and Jayaram, Vinay and Jeunet, Camille and Bakas, Stylianos and Ludwig, Siegfried and Barmpas, Konstantinos and Bahri, Mehdi and Panagakis, Yannis and Laskaris, Nikolaos and Adamos, Dimitrios A. and Zafeiriou, Stefanos and Duong, William C. and Gordon, Stephen M. and Lawhern, Vernon J. and {\'S}liwowski, Maciej and Rouanne, Vincent and Tempczyk, Piotr},
 pages = {205-219},
 abstract = {Transfer learning and meta-learning offer some of the most promising avenues to unlock the scalability of healthcare and consumer technologies driven by biosignal data. This is because regular machine learning methods cannot generalise well across human subjects and handle learning from different, heterogeneously collected data sets, thus limiting the scale of training data available. On the other hand, the many developments in transfer- and meta-learning fields would benefit significantly from a real-world benchmark with immediate practical application. Therefore, we pick electroencephalography (EEG) as an exemplar for all the things that make biosignal data analysis a hard problem. We design two transfer learning challenges around a. clinical diagnostics and b. neurotechnology. These two challenges are designed to probe algorithmic performance with all the challenges of biosignal data, such as low signal-to-noise ratios, major variability among subjects, differences in the data recording sessions and techniques, and even between the specific BCI tasks recorded in the dataset. Task 1 is centred on the field of medical diagnostics, addressing automatic sleep stage annotation across subjects. Task 2 is centred on Brain-Computer Interfacing (BCI), addressing motor imagery decoding across both subjects and data sets. The successful 2021 BEETL competition with its over 30 competing teams and its 3 winning entries brought attention to the potential of deep transfer learning and combinations of set theory and conventional machine learning techniques to overcome the challenges. The results set a new state-of-the-art for the real-world BEETL benchmarks.},
 section = {Competitions}
}

@InProceedings{Gasse22,
 title = {The Machine Learning for Combinatorial Optimization Competition (ML4CO): Results and Insights},
 author = {Gasse, Maxime and Bowly, Simon and Cappart, Quentin and Charfreitag, Jonas and Charlin, Laurent and Ch{\'e}telat, Didier and Chmiela, Antonia and Dumouchelle, Justin and Gleixner, Ambros and Kazachkov, Aleksandr M. and  Khalil, Elias and Lichocki, Pawel and  Lodi, Andrea and Lubin, Miles and Maddison, Chris J. and  Morris Christopher and Papageorgiou, Dimitri J. and  Parjadis, Augustin and Pokutta, Sebastian and Prouvost, Antoine and Scavuzzo, Lara and Zarpellon, Giulia and Yang, Linxin and Lai, Sha and Wang, Akang and   Luo, Xiaodong and Zhou, Xiang and Huang, Haohan and Shao, Shengcheng and Zhu, Yuanming and Zhang, Dong and Quan, Tao and Cao, Zixuan and Xu, Yang and Huang, Zhewei and  Zhou, Shuchang and Binbin, Chen and Minggui, He and Hao, Hao and  Zhiyu, Zhang and Zhiwu, An and Kun, Mao},
 pages = {220-231},
 abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning as a new approach for solving combinatorial problems, either directly as solvers or by enhancing exact solvers. Based on this context, the ML4CO aims at improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components. The competition featured three challenging tasks: finding the best feasible solution, producing the tightest optimality certificate, and giving an appropriate solver configuration. Three realistic datasets were considered: balanced item placement, workload apportionment, and maritime inventory routing. This last dataset was kept anonymous for the contestants.},
 section = {Competitions}
}

@InProceedings{Chang22,
 title = {WebQA: A Multimodal Multihop NeurIPS Challenge},
 author = {Yingshan Chang and Yonatan Bisk},
 pages = {232-245},
 abstract = {Scaling the current QA formulation to the open-domain and multi-hop nature of web searches requires fundamental advances in visual representation learning, multimodal reasoning and language generation. To facilitate research at this intersection, we propose WebQA challenge that mirrors the way humans use the web: 1) Ask a question, 2) Choose sources to aggregate, and 3) Produce a fluent language response. Our challenge for the community is to create unified multimodal reasoning models that can answer questions regardless of the source modality, moving us closer to digital assistants that search through not only text-based knowledge, but also the richer visual trove of information.},
 section = {Competitions}
}

@InProceedings{Weichwald22,
 title = {Learning by Doing: Controlling a Dynamical System using Causality, Control, and Reinforcement Learning},
 author = {Weichwald, Sebastian and Mogensen, S{\o}ren Wengel and Lee, Tabitha Edith and Baumann, Dominik and Kroemer, Oliver and Guyon, Isabelle and Trimpe, Sebastian and Peters, Jonas and Pfister, Niklas},
 pages = {246-258},
 abstract = {Questions in causality, control, and reinforcement learning go beyond the classical machine learning task of prediction under i.i.d. observations. Instead, these fields consider the problem of learning how to actively perturb a system to achieve a certain effect on a response variable. Arguably, they have complementary views on the problem: In control, one usually aims to first identify the system by excitation strategies to then apply model-based design techniques to control the system. In (non-model-based) reinforcement learning, one directly optimizes a reward. In causality, one focus is on identifiability of causal structure. We believe that combining the different views might create synergies and this competition is meant as a first step toward such synergies. The participants had access to observational and (offline) interventional data generated by dynamical systems. Track CHEM considers an open-loop problem in which a single impulse at the beginning of the dynamics can be set, while Track ROBO considers a closed-loop problem in which control variables can be set at each time step. The goal in both tracks is to infer controls that drive the system to a desired state. Code is open-sourced ( https://github.com/LearningByDoingCompetition/learningbydoing-comp ) to reproduce the winning solutions of the competition and to facilitate trying out new methods on the competition tasks.},
 section = {Competitions}
}

@InProceedings{Shah22,
 title = {Retrospective on the 2021 MineRL BASALT Competition on Learning from Human Feedback},
 author = {Shah, Rohin and Wang, Steven H. and Wild, Cody and Milani, Stephanie and Kanervisto, Anssi and Goecks, Vinicius G. and Waytowich, Nicholas and Watkins-Valls, David and Prakash, Bharat and Mills, Edmund and Garg, Divyansh and Fries, Alexander and Souly, Alexandra and Chan, Jun Shern and del Castillo, Daniel and Lieberum, Tom},
 pages = {259-272},
 abstract = {We held the first-ever MineRL Benchmark for Agents that Solve Almost-Lifelike Tasks (MineRL BASALT) Competition at the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021). The goal of the competition was to promote research towards agents that use learning from human feedback (LfHF) techniques to solve open-world tasks. Rather than mandating the use of LfHF techniques, we described four tasks in natural language to be accomplished in the video game Minecraft, and allowed participants to use any approach they wanted to build agents that could accomplish the tasks. Teams developed a diverse range of LfHF algorithms across a variety of possible human feedback types. The three winning teams implemented significantly different approaches while achieving similar performance. Interestingly, their approaches performed well on <em>different</em> tasks, validating our choice of tasks to include in the competition. While the outcomes validated the design of our competition, we did not get as many participants and submissions as our sister competition, MineRL Diamond. We speculate about the causes of this problem and suggest improvements for future iterations of the competition.},
 section = {Competitions}
}

@InProceedings{Nair22,
 title = {Prospective Explanations: An Interactive Mechanism for Model Understanding},
 author = {Nair, Rahul and Tommasi, Pierpaolo},
 pages = {273-277},
 abstract = {We demonstrate a system for prospective explanations of black box models for regression and classification tasks with structured data. Prospective explanations are aimed at showing how models function by highlighting likely changes in model outcomes under changes in input. This in contrast to most post-hoc explanability methods, that aim to provide a justification for a decision retrospectively. To do so, we employ a surrogate Bayesian network model and learn dependencies through a structure learning task. Our system is designed to provide fast estimates of changes in outcomes for any arbitrary exploratory query from users. Such queries are typical partial, i.e. involve only a selected number of features, the outcomes labels are shown therefore as likelihoods. Repeated queries can indicate which aspects of the feature space are more likely to influence the target variable. We demonstrate the system from a real-world application from the humanitarian sector and show the value of bayesian network surrogates.},
 section = {Demonstrations}
}

@InProceedings{Strobelt22,
 title = {Interactive Corpora Visualization for 60 Years of AI Research},
 author = {Strobelt, Hendrik and Hoover, Benjamin},
 pages = {278-282},
 abstract = {Research in artificial intelligence (AI) has been around for over six decades. In that time, it has experienced rich growth, with on and off interest, as researchers tackle this problem from different angles using inspiration from various fields. However, it is difficult to see an overview of the journey that research in AI has taken in its lifespan. We created a visualization we call ``60 Years of AI'' that explores a (biased) selection of the most influential publications that have shaped the field. Our visualization shows similar works clustered together throughout time and allows users to input abstracts of new ideas to see where their ideas position in the landscape of the ever-growing field of AI.},
 section = {Demonstrations}
}

@InProceedings{Gruppi22,
 title = {The SenSE Toolkit: A System for Visualization and Explanation of Semantic Shift},
 author = {Gruppi, Maur\'{i}cio and Adal\i, Sibel and Chen, Pin-Yu},
 pages = {283-287},
 abstract = { Lexical Semantic Change (LSC) detection, also known as Semantic Shift, is the process of identifying and characterizing variations in language usage across different scenarios such as time and domain. It allows us to track the evolution of word senses, as well as to understand the difference between the languages used by distinct communities. LSC detection is often done by applying a distance measure over vectors of two aligned word embedding matrices. In this paper, we present SenSE, an interactive semantic shift exploration toolkit that provides visualization and explanation of lexical semantic change for an input pair of text sources. Our system focuses on showing how the different alignment strategies may affect the output of an LSC model as well as on explaining semantic change based on the neighbors of a chosen target word, while also extracting examples of sentences where these semantic deviations appear. The system runs as a web application (available at \url{http://sense.mgruppi.me}), allowing the audience to interact by configuring the alignment strategies while visualizing the results in a web browser.},
 section = {Demonstrations}
}

@InProceedings{Cornec22,
 title = {AIMEE: Interactive model maintenance with rule-based surrogates},
 author = {Cornec, Owen and Nair, Rahul and Daly, Elizabeth and Alkan, Oznur and Wei, Dennis},
 pages = {288-291},
 abstract = {In real-world applications, such as loan approvals or claims management, machine learn-ing (ML) models need to be updated or retrained to adhere to new rules and regulations.But how can a new model be built and new decision boundaries be formed without having new training data available? We present the AI Model Explorer and Editor tool (AIMEE) for model exploration and model editing using human understandable rules. It addresses the problem of changing decision boundaries by leveraging user-specified feedback rules that are used to pre-process training data such that a retrained model will reflect user changes.The pre-processing step uses synthetic oversampling and relabeling and assumes black-box access to the algorithm that retrains the model. AIMEE provides interactive methods to edit rule sets, visualize changes to decision boundaries, and generate interpretable comparisons of model changes so that users see their feedback reflected in the updated model. The demo shows an end-to-end solution that supports the full update lifecycle of an ML model.},
 section = {Demonstrations}
}

@InProceedings{Jiang22,
 title = {GANs for All: Supporting Fun and Intuitive Exploration of GAN Latent Spaces},
 author = {Jiang, Wei and Davis, Richard Lee and Kim, Kevin Gonyop and Dillenbourg, Pierre},
 pages = {292-296},
 abstract = {We have developed a new tool that makes it possible for people with zero programming experience to intentionally and meaningfully explore the latent space of a GAN. We combine a number of methods from the literature into a single system that includes multiple functionalities: uploading and locating images in the latent space, image generation with text, visual style mixing, and intentional and intuitive latent space exploration. This tool was developed to provide a means for designers to explore the "design space" of their domains. Our goal was to create a system to support novices in gaining a more complete, expert understanding of their domain{'}s design space by lowering the barrier of entry to using deep generative models in creative practice.},
 section = {Demonstrations}
}

@InProceedings{Hadgu22,
 title = {Lesan {--} Machine Translation for Low Resource Languages},
 author = {Hadgu, Asmelash Teka and Aregawi, Abel and Beaudoin, Adam},
 pages = {297-301},
 abstract = {Millions of people around the world can not access content on the Web because most of the content is not readily available in their language. Machine translation (MT) systems have the potential to change this for many languages. Current MT systems provide very accurate results for high resource language pairs, e.g., German and English. However, for many low resource languages, MT is still under active research. The key challenge is lack of datasets to build these systems. We present Lesan (https://lesan.ai/), an MT system for low resource languages. Our pipeline solves the key bottleneck to low resource MT by leveraging online and offline sources, a custom Optical Character Recognition (OCR) system for Ethiopic and an automatic alignment module. The final step in the pipeline is a sequence to sequence model that takes parallel corpus as input and gives us a translation model. Lesan{'}s translation model is based on the Transformer architecture. After constructing a base model, back translation is used to leverage monolingual corpora. Currently Lesan supports translation to and from Tigrinya, Amharic and English. We perform extensive human evaluation and show that Lesan outperforms state-of-the-art systems such as Google Translate and Microsoft Translator across all six pairs. Lesan is freely available and has served more than 10 million translations so far. At the moment, there are only 217 Tigrinya and 15,009 Amharic Wikipedia articles. We believe that Lesan will contribute towards democratizing access to the Web through MT for millions of people.},
 section = {Demonstrations}
}

@InProceedings{Datta22,
 title = {Exploring Conceptual Soundness with TruLens},
 author = {Datta, Anupam and Fredrikson, Matt and Leino, Klas and Lu, Kaiji and Sen, Shayak and Shih, Ricardo and Wang, Zifan},
 pages = {302-307},
 abstract = {As machine learning has become increasingly ubiquitous, there has been a growing need to assess the trustworthiness of learned models. One important aspect to model trust is conceptual soundness, i.e., the extent to which a model uses features that are appropriate for its intended task. We present <em>TruLens</em>, a new cross-platform framework for explaining deep network behavior. In our demonstration, we provide an interactive application built on TruLens that we use to explore the conceptual soundness of various pre-trained models. We take the unique perspective that robustness to small-norm adversarial examples is a necessary condition for conceptual soundness; we demonstrate this by comparing explanations on models trained with and without a robust objective. Our demonstration will focus on our end-to-end application, which will be made accessible for the audience to interact with; but we will also provide details on its open-source components, including the TruLens library and the code used to train robust networks.},
 section = {Demonstrations}
}

@InProceedings{Cai22,
 title = {Real-Time and Accurate Self-Supervised Monocular Depth Estimation on Mobile Device},
 author = {Cai, Hong and Yin, Fei and Singhal, Tushar and Pendyam, Sandeep and Noorzad, Parham and Zhu, Yinhao and Nguyen, Khoi and Matai, Janarbek and Ramaswamy, Bharath and Mayer, Frank and Patel, Chirag and Khobare, Abhijit and Porikli, Fatih},
 pages = {308-313},
 abstract = {In this paper, we present our innovations on self-supervised monocular depth estimation. First, we enhance self-supervised monocular depth estimation with semantic information during training. This reduces the error by 12\% and achieves state-of-the-art performance. Second, we enhance the backbone architecture using a scalable method for neural architecture search which optimizes directly for inference latency on a target device. This enables operation at more than 30 FPS. We demonstrate these techniques on a smartphone powered by a Snapdragon Mobile Platform.},
 section = {Demonstrations}
}

@InProceedings{Kumar22,
 title = {Automated Evaluation of GNN Explanations with Neuro Symbolic Reasoning},
 author = {Kumar, Vanya Bannihatti and Ganesan, Balaji and Ameen, Muhammed and Sharma, Devbrat and Agarwal, Arvind},
 pages = {314-318},
 abstract = {Explaining Graph Neural Networks predictions to end users of AI applications in easily understandable terms remains an unsolved problem. In particular, we do not have well developed methods for automatically evaluating explanations, in ways that are closer to how users consume those explanations. Based on recent application trends and our own experiences in real world problems, we propose an automatic evaluation approach for GNN Explanations using Neuro Symbolic Reasoning.},
 section = {Demonstrations}
}

@InProceedings{Ahmed22,
 title = {Pylon: A PyTorch Framework for Learning with Constraints},
 author = {Ahmed, Kareem and Li, Tao and Ton, Thy and Guo, Quan and Chang, Kai-Wei and Kordjamshidi, Parisa and Srikumar, Vivek and Van den Broeck, Guy and Singh, Sameer},
 pages = {319-324},
 abstract = {Deep learning excels at learning low-level task information from large amounts of data, but struggles with learning high-level domain knowledge, which can often be directly and succinctly expressed. In this work, we introduce Pylon, a neuro-symbolic training framework that builds on PyTorch to augment procedurally trained neural networks with declaratively specified knowledge. Pylon allows users to programmatically specify <em>constraints<\em> as PyTorch functions, and compiles them into a differentiable loss, thus training predictive models that fit the data <em>whilst<\em> satisfying the specified constraints. Pylon includes both exact as well as approximate compilers to efficiently compute the loss, employing fuzzy logic, sampling methods, and circuits, ensuring scalability even to complex models and constraints. A guiding principle in designing Pylon has been the ease with which any existing deep learning codebase can be extended to learn from constraints using only a few lines: a function expressing the constraint and a single line of code to compile it into a loss. We include case studies from natural language processing, computer vision, logical games, and knowledge graphs, that can be interactively trained, and highlights Pylon{'}s usage.},
 section = {Demonstrations}
}

@InProceedings{Ford22,
 title = {MEWS: Real-time Social Media Manipulation Detection and Analysis},
 author = {Ford, Trenton and Yankoski, Michael and Theisen, William and Henry, Tom and Khashman, Farah and Dearstyne, Katherine and Weninger, Tim and Bilo Thomas, Pamela},
 pages = {325-329},
 abstract = {This article presents a beta-version of MEWS (Misinformation Early Warning System). It describes the various aspects of the ingestion, manipulation detection, and graphing algorithms employed to determine--in near real-time--the relationships between social media images as they emerge and spread on social media platforms. By combining these various technologies into a single processing pipeline, MEWS can identify manipulated media items as they arise and identify when these particular items begin trending on individual social media platforms or even across multiple platforms. The emergence of a novel manipulation followed by rapid diffusion of the manipulated content suggests a disinformation campaign.},
 section = {Demonstrations}
}

@InProceedings{Rathore22,
 title = {An Interactive Visual Demo of Bias Mitigation Techniques for Word Representations From a Geometric Perspective},
 author = {Rathore, Archit and Dev, Sunipa and Srikumar, Vivek and Phillips, Jeff M and Zheng, Yan and Yeh, Michael and Wang, Junpeng and Zhang, Wei and Wang, Bei},
 pages = {330-335},
 abstract = {Language representations are known to encode and propagate biases, i.e., stereotypical associations between words or groups of words that may cause representational harm. In this demo, we utilize interactive visualization to increase the interpretability of a number of state-of-the-art techniques that are designed to identify, mitigate, and attenuate these biases in word representations, in particular, from a geometric perspective. We provide an open source web-based visualization tool and offer hands-on experience in exploring the effects of these debiasing techniques on the geometry of high-dimensional word vectors. To help understand how various debiasing techniques change the underlying geometry, we decompose each technique into modular and interpretable sequences of primitive operations, and study their effect on the word vectors using dimensionality reduction and interactive visual exploration. This demo is primarily designed to aid natural language processing (NLP) practitioners and researchers working with fairness and ethics of machine learning systems. It can also be used to educate NLP novices in understanding the existence of and then mitigating biases in word embeddings. },
 section = {Demonstrations}
}

@InProceedings{Borzunov22,
 title = {Training Transformers Together},
 author = {Borzunov, Alexander and Ryabinin, Max and Dettmers, Tim and Lhoest, Quentin and Saulnier, Lucile and Diskin, Michael and Jernite, Yacine},
 pages = {335-342},
 abstract = {The infrastructure necessary for training state-of-the-art models is becoming overly expensive, which makes training such models affordable only to large corporations and institutions. Recent work proposes several methods for training such models collaboratively, i.e., by pooling together hardware from many independent parties and training a shared model over the Internet. In this demonstration, we collaboratively trained a text-to-image transformer similar to OpenAI DALL-E. We invited the viewers to join the ongoing training run, showing them instructions on how to contribute using the available hardware. We explained how to address the engineering challenges associated with such a training run (slow communication, limited memory, uneven performance between devices, and security concerns) and discussed how the viewers can set up collaborative training runs themselves. Finally, we show that the resulting model generates images of reasonable quality on a number of prompts.},
 section = {Demonstrations}
}

@InProceedings{Gharibi22,
 title = {TripleBlind: A Privacy-Preserving Framework for Decentralized Data and Algorithms},
 author = {Gharibi, Gharib and Poorebrahim Gilkalaye, Babak and Patel, Ravi and Rademacher, Andrew and Wagner, David and Fay, Jack and Moore, Gary and Penrod, Steve and Storm, Greg and Das, Riddhiman},
 pages = {343-348},
 abstract = {Centralized sharing of sensitive data for training and inference is challenging and undesired due to privacy, competition, and legal concerns. While distributed learning and secure inference have demonstrated significant privacy gains, these methods still largely ignore the design and implementation of practical, privacy-preserving tool support. To address these challenges, we present TripleBlind, an automated privacy-preserving framework for creating and consuming data-driven applications from decentralized data and algorithms. TripleBlind provides a set of automated, high-level APIs that enable (1) extracting knowledge from remote data without moving it outside the owner's infrastructure, (2) training AI models from decentralized data, and (3) consuming trained models for secure inference-as-a-service; all without compromising the privacy of either the model/query or the data. In this short paper, we shed light on the underlying training and inference methods, the design and implementation of our framework, and showcase the actual code necessary to run a secure, remote inference using our secure multi-party computation API. A video demo highlighting the main features of our framework is located at www.tripleblind.ai/neurips2021 },
 section = {Demonstrations}
}

@InProceedings{Zanjani22,
 title = {Deep Learning Frameworks for Weakly-Supervised Indoor Localization},
 author = {Farhad G. Zanjani and Ilia Karmanov and Hanno Ackermann and Daniel Dijkman and Simone Merlin and Ishaque Kadampot and Brian Buesker and Vamsi Vegunta and Fatih Porikli},
 pages = {349-354},
 abstract = {We present two weakly-supervised deep learning frameworks for person indoor localization through Wi-Fi signal. These two frameworks, namely OT-Isomap and WiCluster, in contrast with prior works, require only room/zone level labels that is easier to acquire, compared to hard-to-acquire centimeter accuracy position labels. The OT-Isomap is a modality-agnostic model and formulates the localization problem in the context of parametric manifold learning and optimal transportation. This framework allows jointly learning a low-dimensional embedding as well as correspondences with a topological map. The WiCluster method is based on self-supervised deep clustering and metric learning models. Inspired by the deep cluster method, the Wi-Fi signals are spatially charted and represented in lower-dimensional space while a triplet margin-loss constrains an isometric representation of data on its 2D/3D intrinsic space. We demonstrate the meter-level accuracy of these two methods on both real-world Wi-Fi and camera-based indoor localization.},
 section = {Demonstrations}
}


@InProceedings{Jiang22b,
 title = {AME: Interpretable Almost Exact Matching for Causal Inference}
 author = {Jiang, Haoning and Howell, Thomas and Gupta, Neha R. and Orlandi, Vittorio and Morucci, Marco and Parikh, Harsh and Roy, Sudeepa and Rudin, Cynthia and Volfovsky, Alexander},
 pages = {355-359},
 abstract = {AME-GUI (Almost Matching Exactly Graphical User Interface) is an interactive web- based application that allows users to perform matching for causal inference on large, complex datasets with categorical covariates. The application is powered by the Fast Large-Scale Almost Matching Exactly (FLAME) algorithm (Wang et al., 2021), which matches treatment and control units in a way that is (i) interpretable, because the matches are made directly on covariates, (ii) high-quality, because machine learning is used to determine which covariates are important to match on, and (iii) scalable, using techniques from data management. The graphical user interface highlights the utility of this algorithm and uses a suite of visualization tools to facilitate easy and interactive exploration of treatment effect estimates, as well as of the created matched groups that they depend on. The application gives a quick and simple overview of the open-source Python and R packages dame-flame and FLAME, and the range of functionality they provide for interpretable and efficient causal inference.},
 section = {Demonstrations}
}
